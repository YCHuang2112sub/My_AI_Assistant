{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "##### Speech concatenate continuous chunks of audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!python stt_t2t_no_queue.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import atexit\n",
    "import sounddevice as sd\n",
    "import noisereduce as nr\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "\n",
    "# list1 = []\n",
    "\n",
    "# def foo(list1, q):\n",
    "#     q.put('hello')\n",
    "#     list1.append('hello')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # mp.set_start_method('spawn')\n",
    "#     q = mp.Queue()\n",
    "#     p = mp.Process(target=foo, args=(list1,q))\n",
    "#     p.start()\n",
    "#     # print(q.get(False))\n",
    "#     p.join()\n",
    "#     print(q.get_nowait())\n",
    "\n",
    "    \n",
    "#     # time.sleep(5)\n",
    "#     # print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for audio stream configuration\n",
    "FORMAT = pyaudio.paInt16  # Sample format (16-bit)\n",
    "CHANNELS = 1             # Number of audio channels (1 for mono, 2 for stereo)\n",
    "# RATE = 44100             # Sample rate (samples per second)\n",
    "# CHUNK_SIZE = 44100        # Number of audio frames per buffer\n",
    "CHUNK_SIZE = 16000        # Number of audio frames per buffer\n",
    "SAMPLING_RATE = 16000     # Sample rate (samples per second)\n",
    "\n",
    "DETECT_THRESHOLD = 1000   # Threshold to detect a clap\n",
    "DETECT_DURATION = 0.1    # Time in seconds to detect a clap\n",
    "RECORD_DURATION = 3      # Time in seconds to record audio after clap is detected\n",
    "MAX_RECORD_DURATION = 20 # Maximum time in seconds to record audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "# model = whisper.load_model(\"medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(audio):\n",
    "async def inference(audio):\n",
    "    if type(audio) == str:\n",
    "        audio = whisper.load_audio(audio)\n",
    "    print(type(audio), audio.dtype, audio.shape)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    \n",
    "    # _, probs = model.detect_language(mel)\n",
    "    \n",
    "    options = whisper.DecodingOptions(fp16 = False)\n",
    "    result = whisper.decode(model, mel, options)\n",
    "    \n",
    "    print(result.text)\n",
    "    return result.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_filter(text):\n",
    "    \n",
    "    char_count = Counter(text)\n",
    "    # print(char_count)\n",
    "    n_chars = len(char_count.keys())\n",
    "    print(\"n_chars = \", n_chars)\n",
    "    flag_n_chars_too_little = n_chars < 4\n",
    "    # flag_any_char_too_many = np.all(np.array(list(char_count.values())) > 20)\n",
    "\n",
    "    # return flag_n_chars_too_little and flag_any_char_too_many\n",
    "    return flag_n_chars_too_little "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput import keyboard\n",
    "\n",
    "def on_press(key):\n",
    "    if key == keyboard.Key.esc:\n",
    "        # Stop listener\n",
    "        return False\n",
    "\n",
    "def on_release(key):\n",
    "    pass\n",
    "\n",
    "# ...or, in a non-blocking fashion:\n",
    "listener = keyboard.Listener(\n",
    "    on_press=on_press,\n",
    "    on_release=on_release)\n",
    "listener.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stt_func(audio_data):\n",
    "    text = await inference(audio_data)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def playback(audio_data, samplerate):\n",
    "# def playback(audio_data, samplerate):\n",
    "    sd.play(audio_data.astype(float)/4096, samplerate=samplerate)\n",
    "    # raise Exception(\"not consumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=SAMPLING_RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "\n",
    "# stream.stop_stream()\n",
    "# stream.close()\n",
    "# audio.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    data = stream.read(int(DETECT_DURATION * SAMPLING_RATE), exception_on_overflow=False)\n",
    "    audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "    # print(audio_data_in[:20])\n",
    "    # print(\"detecting:\", np.max(audio_data_in))\n",
    "\n",
    "    if np.max(audio_data_in) < DETECT_THRESHOLD:\n",
    "        continue\n",
    "    \n",
    "    print(np.max(audio_data_in))\n",
    "\n",
    "    audio_data = audio_data_in\n",
    "    while np.max(audio_data_in) > DETECT_THRESHOLD:\n",
    "        data = stream.read(int(RECORD_DURATION * SAMPLING_RATE), exception_on_overflow=False)\n",
    "        audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "        audio_data = np.concatenate((audio_data, audio_data_in))\n",
    "        if(len(audio_data) > SAMPLING_RATE*30):\n",
    "            break\n",
    "\n",
    "    # data = stream.read(int(RECORD_DURATION * SAMPLING_RATE))\n",
    "    # audio_data_record = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "    # audio_data = np.concatenate((audio_data_in, audio_data_record))\n",
    "    await playback(audio_data, SAMPLING_RATE)\n",
    "    # audio_data = audio_data_in\n",
    "    \n",
    "    print(\"audio_data min,max = \", np.min(audio_data), np.max(audio_data), audio_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # # Reduce noise from the audio using noisereduce\n",
    "    # audio_noise_reduced = nr.reduce_noise(y=audio_data, sr=SAMPLING_RATE)\n",
    "    \n",
    "    # sd.play(audio_data, samplerate=SAMPLING_RATE)\n",
    "\n",
    "    # print(\"audio_noise_reduced min,max = \", np.min(audio_noise_reduced), np.max(audio_noise_reduced), audio_noise_reduced.shape)\n",
    "\n",
    "    # await stt_func(audio_data.astype(np.float32))\n",
    "    \n",
    "    # text = inference(audio_noise_reduced)\n",
    "    # text = inference(audio_noise_reduced.astype(np.float32))\n",
    "    # text = inference(audio_data.astype(np.float32))\n",
    "\n",
    "    # flag_skip = text_filter(text)\n",
    "\n",
    "    # if flag_skip:\n",
    "    #     print(\"skip +: \", text)\n",
    "    #     continue\n",
    "\n",
    "    # print(text)\n",
    "    \n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=SAMPLING_RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "\n",
    "    if not listener.running:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a coroutine\n",
    "# coro = asyncio.sleep(1, result=3)\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# timeout = 5\n",
    "# # Submit the coroutine to a given loop\n",
    "# future = asyncio.run_coroutine_threadsafe(coro, loop)\n",
    "\n",
    "# # Wait for the result with an optional timeout argument\n",
    "# assert future.result(timeout) == 3\n",
    "\n",
    "# try:\n",
    "#     result = future.result(timeout)\n",
    "# except TimeoutError:\n",
    "#     print('The coroutine took too long, cancelling the task...')\n",
    "#     future.cancel()\n",
    "# except Exception as exc:\n",
    "#     print(f'The coroutine raised an exception: {exc!r}')\n",
    "# else:\n",
    "#     print(f'The coroutine returned: {result!r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess\n",
    "# subprocess.Popen([\"dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process, Manager\n",
    "\n",
    "# def f(d, l):\n",
    "#     d[1] = '1'\n",
    "#     d['2'] = 2\n",
    "#     d[0.25] = None\n",
    "#     l.reverse()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     with Manager() as manager:\n",
    "#         d = manager.dict()\n",
    "#         l = manager.list(range(10))\n",
    "\n",
    "#         p = Process(target=f, args=(d, l))\n",
    "#         p.start()\n",
    "#         p.join()\n",
    "\n",
    "#         print(d)\n",
    "#         print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import concurrent.futures\n",
    "# import threading\n",
    "# import time\n",
    "# import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # loop = asyncio.get_running_loop()\n",
    "\n",
    "# # IO_bound -> thread pool\n",
    "# # CPU_bound -> process pool\n",
    "\n",
    "# # # 3. Run in a custom process pool:\n",
    "# # # with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "# # with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "# #     result = await loop.run_in_executor(\n",
    "# #         pool, playback)\n",
    "# #     print('custom process pool', result)\n",
    "# # asyncio.create_task(playback(audio_data, SAMPLING_RATE))\n",
    "# await playback(audio_data, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def playback(audio_data, samplerate):\n",
    "def playback(audio_data, samplerate):\n",
    "    sd.play(audio_data.astype(float)/4096, samplerate=samplerate)\n",
    "    # raise Exception(\"not consumed\")\n",
    "    \n",
    "import time\n",
    "audio_data = np.zeros(1)\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=SAMPLING_RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "for i in range(3):\n",
    "    data = stream.read(int(RECORD_DURATION * SAMPLING_RATE))\n",
    "    # data = stream.read(int(DETECT_DURATION * SAMPLING_RATE), exception_on_overflow=False)\n",
    "    # audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "    audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "    audio_data = np.concatenate((audio_data, audio_data_in))\n",
    "    \n",
    "    # asyncio.create_task(playback(audio_data, SAMPLING_RATE))\n",
    "    # await playback(audio_data, SAMPLING_RATE)\n",
    "    # threading.Thread(target=playback, args=(audio_data, SAMPLING_RATE), daemon=True).start()\n",
    "    mp = multiprocessing.Process(target=playback, args=(audio_data, SAMPLING_RATE), daemon=True)\n",
    "    mp.start()\n",
    "    mp.join()\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play audio_data\n",
    "# sd.play(audio_data.astype(float)/4096, samplerate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import SpeechT5ForTextToSpeech, SpeechT5HifiGan, SpeechT5Processor, pipeline\n",
    "from transformers import VitsModel, VitsTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path =\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/checkpoint_88000.pth\"  # Absolute path to the model checkpoint.pth\n",
    "config_path =\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/config.json\" # Absolute path to the model config.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_input_queue = Queue(maxsize=20)\n",
    "audio_input_queue = Queue()\n",
    "audio_speech2text_queue = Queue()\n",
    "stop_record_event = threading.Event()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your constants (FORMAT, CHANNELS, RATE, CHUNK_SIZE, RECORD_DURATION, DETECT_THRESHOLD)\n",
    "\n",
    "# def audio_processing():\n",
    "    # audio = pyaudio.PyAudio()\n",
    "    # stream = audio.open(format=FORMAT, channels=CHANNELS, rate=SAMPLING_RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    # while True:\n",
    "    #     data = stream.read(int(DETECT_DURATION * SAMPLING_RATE))\n",
    "    #     audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "    #     # print(audio_data_in[:20])\n",
    "    #     # print(\"detecting:\", np.max(audio_data_in))\n",
    "\n",
    "    #     if np.max(audio_data_in) < DETECT_THRESHOLD:\n",
    "    #         continue\n",
    "        \n",
    "    #     print(np.max(audio_data_in))\n",
    "\n",
    "    #     audio_data = audio_data_in\n",
    "        \n",
    "    #     print(\"audio_data min,max = \", np.min(audio_data), np.max(audio_data), audio_data.shape)\n",
    "\n",
    "    #     # Reduce noise from the audio using noisereduce\n",
    "    #     audio_noise_reduced = nr.reduce_noise(y=audio_data, sr=SAMPLING_RATE)\n",
    "        \n",
    "    #     # sd.play(audio_noise_reduced, samplerate=SAMPLING_RATE)\n",
    "\n",
    "    #     print(\"audio_noise_reduced min,max = \", np.min(audio_noise_reduced), np.max(audio_noise_reduced), audio_noise_reduced.shape)\n",
    "\n",
    "    #     # while np.max(audio_data_in) >= DETECT_THRESHOLD:\n",
    "    #     #     data = stream.read(int(RECORD_DURATION * RATE))\n",
    "    #     #     audio_data_in = np.frombuffer(data, dtype=np.int16)\n",
    "    #     #     print(\"input voice max = \", np.max(audio_data_in), audio_data_in.shape)\n",
    "    #     #     audio_data = np.concatenate((audio_data, audio_data_in))\n",
    "\n",
    "    #     # print(\"audio_data.shape = \", audio_data.shape)\n",
    "    #     audio_input_queue.put(audio_noise_reduced)\n",
    "        \n",
    "\n",
    "# # Create and start the thread as a daemon\n",
    "# audio_input_thread = threading.Thread(target=audio_processing, daemon=True)\n",
    "# audio_input_thread.start()\n",
    "\n",
    "# # You can continue with other tasks here...\n",
    "\n",
    "# # Optionally, you can wait for the audio thread to finish if needed\n",
    "# # audio_input_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if not audio_input_queue.empty():\n",
    "        audio_data = np.zeros((1,))\n",
    "        # for i in range(10):\n",
    "        audio_input = audio_input_queue.get()\n",
    "        audio_input = np.concatenate((audio_input, audio_input_queue.get()))\n",
    "        sd.play(audio_input, samplerate=SAMPLING_RATE)\n",
    "        # sd.wait()\n",
    "        audio_input_queue.task_done()\n",
    "\n",
    "    if not listener.running:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def audio_playing():\n",
    "#     while(True):    \n",
    "#         audio_data = audio_input_queue.get()\n",
    "#         # print(\"C: data = \", data)\n",
    "#         # data_with_id = f\"Task C: {data}\"\n",
    "#         print(\"receive audio data, len = \", len(audio_data), \"type = \", type(audio_data))\n",
    "#         sd.play(audio_data, SAMPLING_RATE)\n",
    "#         # sd.wait()\n",
    "#         audio_input_queue.task_done()\n",
    "\n",
    "#         if not listener.running:\n",
    "#             break\n",
    "\n",
    "# # Create and start the thread as a daemon\n",
    "# audio_playing_thread = threading.Thread(target=audio_playing, daemon=True)\n",
    "# audio_playing_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_speech2text():\n",
    "    while(True):    \n",
    "        audio_data = audio_input_queue.get()\n",
    "        print(\"stt: received audio data = \", len(audio_data))\n",
    "        # data_with_id = f\"Task C: {data}\"\n",
    "        audio_speech2text = inference(audio_data.astype(np.float32))\n",
    "        # sd.play(audio_data, SAMPLING_RATE)\n",
    "        # sd.wait()\n",
    "        audio_speech2text_queue.put(audio_speech2text)\n",
    "        audio_input_queue.task_done()\n",
    "\n",
    "        if not listener.running:\n",
    "            break\n",
    "\n",
    "# Create and start the thread as a daemon\n",
    "audio_speech2text_thread = threading.Thread(target=audio_speech2text, daemon=True)\n",
    "audio_speech2text_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_printing():\n",
    "    while(True):    \n",
    "        text = audio_speech2text_queue.get()\n",
    "        # print(\"C: data = \", data)\n",
    "        # data_with_id = f\"Task C: {data}\"\n",
    "        print(\"receive text data, len = \", len(text), text)\n",
    "        audio_speech2text_queue.task_done()\n",
    "\n",
    "        if not listener.running:\n",
    "            break\n",
    "\n",
    "# Create and start the thread as a daemon\n",
    "text_printing_thread = threading.Thread(target=text_printing, daemon=True)\n",
    "text_printing_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Function to forcefully terminate all threads\n",
    "def force_terminate_threads():\n",
    "    sys.exit()\n",
    "    audio_input_thread.join()\n",
    "    audio_playing_thread.join()\n",
    "\n",
    "# Register the force_terminate_threads function to be called at program exit\n",
    "atexit.register(force_terminate_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import SpeechT5ForTextToSpeech, SpeechT5HifiGan, SpeechT5Processor, pipeline\n",
    "from transformers import VitsModel, VitsTokenizer\n",
    "\n",
    "# from TTS.config import load_config\n",
    "# from TTS.utils.manage import ModelManager\n",
    "# from TTS.utils.synthesizer import Synthesizer\n",
    "# import torchaudio\n",
    "# from TTS.api import TTS\n",
    "\n",
    "\n",
    "# tts=TTS(model_path=\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/checkpoint_88000.pth\",\n",
    "#         config_path=\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/config.json\")\n",
    "\n",
    "model_path =\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/checkpoint_88000.pth\"  # Absolute path to the model checkpoint.pth\n",
    "config_path =\"https://huggingface.co/Kamtera/persian-tts-male1-vits/resolve/main/config.json\" # Absolute path to the model config.json\n",
    "\n",
    "# text_fa=\".زندگی فقط یک بار است؛ از آن به خوبی استفاده کن\"\n",
    "\n",
    "\n",
    "# synthesizer = Synthesizer(\n",
    "#     model_path, config_path\n",
    "# )\n",
    "\n",
    "# synthesizer = Synthesizer(\n",
    "#     model_path, config_path\n",
    "# )\n",
    "# wavs = synthesizer.tts(text_fa)\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load speech translation checkpoint\n",
    "asr_pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device)\n",
    "\n",
    "# load text-to-speech checkpoint and speaker embeddings\n",
    "# processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "# model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "# speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "# model = VitsModel.from_pretrained(\"Matthijs/mms-tts-deu\").to(device)\n",
    "model = VitsModel.from_pretrained(\"Matthijs/mms-tts-deu\")\n",
    "tokenizer = VitsTokenizer.from_pretrained(\"Matthijs/mms-tts-deu\")\n",
    "\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# en_fa_trans_model_size = \"base\"\n",
    "# en_fa_trans_model_name = f\"persiannlp/mt5-{en_fa_trans_model_size}-parsinlu-translation_en_fa\"\n",
    "# en_fa_trans_tokenizer = MT5Tokenizer.from_pretrained(en_fa_trans_model_name)\n",
    "# en_fa_trans_model = MT5ForConditionalGeneration.from_pretrained(en_fa_trans_model_name)\n",
    "\n",
    "# def translate_en2fa(input_string, **generator_args):\n",
    "#     input_ids = en_fa_trans_tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "#     res = en_fa_trans_model.generate(input_ids, **generator_args)\n",
    "#     output = en_fa_trans_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "#     print(input_string, output)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# translate_en2fa(\"Praise be to Allah, the Cherisher and Sustainer of the worlds;\")\n",
    "\n",
    "\n",
    "def translate(audio):\n",
    "    # outputs = asr_pipe(audio, max_new_tokens=256, generate_kwargs={\"task\": \"translate\"})\n",
    "    # # outputs = asr_pipe(audio, max_new_tokens=256, generate_kwargs={\"language\":\"<|fa|>\",\"task\": \"transcribe\"})\n",
    "    outputs = asr_pipe(audio, max_new_tokens=256, generate_kwargs={\"language\":\"<|de|>\",\"task\": \"transcribe\"})\n",
    "    return outputs[\"text\"]\n",
    "\n",
    "\n",
    "def synthesise(text):\n",
    "    # inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    # speech = model.generate_speech(inputs[\"input_ids\"].to(device), speaker_embeddings.to(device), vocoder=vocoder)\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # inputs = [x.to(device) for x in inputs]\n",
    "    # input_ids = inputs[\"input_ids\"].to(device)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        # outputs = model(input_ids)\n",
    "        output = model(**inputs).waveform\n",
    "\n",
    "    sample_rate=model.config.sampling_rate\n",
    "    speech=output.cpu().numpy()\n",
    "    \n",
    "    # speech = outputs.audio[0]\n",
    "\n",
    "    \n",
    "    # tts.tts_to_file(text,file_path='output_fa.wav')\n",
    "    # outputs, sample_rate = torchaudio.load(\"output_fa.wav\")\n",
    "    # speech = outputs[0]\n",
    "\n",
    "    # wavs = synthesizer.tts(text)\n",
    "    # speech = torch.tensor(wavs)[None]\n",
    "    # sample_rate = 22050\n",
    "\n",
    "    \n",
    "    return speech, sample_rate\n",
    "\n",
    "\n",
    "def speech_to_speech_translation(audio):\n",
    "    translated_text = translate(audio)\n",
    "    # print(translated_text)\n",
    "    # translated_text = translate_en2fa(translated_text)\n",
    "    # translated_text = str(translated_text)\n",
    "    # print(translated_text)\n",
    "    synthesised_speech, sample_rate = synthesise(translated_text)\n",
    "    # print(synthesised_speech.shape, sample_rate)\n",
    "    # print(synthesised_speech.max(), synthesised_speech.min())\n",
    "    # synthesised_speech = (synthesised_speech.numpy() * 32767).astype(np.int16)\n",
    "    synthesised_speech = ( (synthesised_speech - synthesised_speech.min()) * 10000).astype(np.int16)[0]\n",
    "      \n",
    "    # print(synthesised_speech.shape, sample_rate)\n",
    "    # print(synthesised_speech.max(), synthesised_speech.min())\n",
    "\n",
    "    # import sounddevice as sd\n",
    "    # sd.play(synthesised_speech, samplerate=sample_rate)\n",
    "    # sd.wait()\n",
    "\n",
    "    return sample_rate, synthesised_speech\n",
    "\n",
    "\n",
    "title = \"Cascaded STST\"\n",
    "description = \"\"\"\n",
    "Demo for cascaded speech-to-speech translation (STST), mapping from source speech in any language to target speech in English. Demo uses OpenAI's [Whisper Base](https://huggingface.co/openai/whisper-base) model for speech translation, and Microsoft's\n",
    "[SpeechT5 TTS](https://huggingface.co/microsoft/speecht5_tts) model for text-to-speech:\n",
    "![Cascaded STST](https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/s2st_cascaded.png \"Diagram of cascaded speech to speech translation\")\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_translate = gr.Interface(\n",
    "    fn=speech_to_speech_translation,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.Audio(label=\"Generated Speech\", type=\"numpy\"),\n",
    "    title=title,\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "file_translate = gr.Interface(\n",
    "    fn=speech_to_speech_translation,\n",
    "    inputs=gr.Audio(source=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.Audio(label=\"Generated Speech\", type=\"numpy\"),\n",
    "    # examples=[[\"./example.wav\"]],\n",
    "    title=title,\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "with demo:\n",
    "    gr.TabbedInterface([mic_translate, file_translate], [\"Microphone\", \"Audio File\"])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
